{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f3da1e",
   "metadata": {},
   "source": [
    "# DS637 Midterm Project — Equity Portfolio Management (2018)\n",
    "**Student:** Umair Ali\n",
    "\n",
    "This notebook includes **robust file loading fixes** for cases where your downloaded “CSV” files are actually saved as **Excel** files on Windows (e.g., File Explorer shows “Microsoft Excel …”).\n",
    "\n",
    "✅ Fixes included:\n",
    "- Loads files whether they are `.csv`, `.xlsx`, `.xls`, or have **no visible extension**\n",
    "- Avoids `UnicodeDecodeError` by falling back to `read_excel()` when needed\n",
    "- Parses dates consistently (prefers `YYYY-MM-DD`)\n",
    "- Handles USDJPY file the same robust way\n",
    "\n",
    "> Put all files in the **same folder** as this notebook, or update `data_dir`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcee179",
   "metadata": {},
   "source": [
    "## 0) Setup + robust loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b6246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dir = Path('.')  # <-- folder where your files are\n",
    "\n",
    "universe = ['IBM','MSFT','GOOG','AAPL','AMZN','META','NFLX','TSLA','ORCL','SAP']\n",
    "\n",
    "def find_file_for_key(key: str, folder: Path) -> Path:\n",
    "    \"\"\"Find a file for a ticker or key (like 'usdjpy') by searching the folder.\"\"\"\n",
    "    key_l = key.lower()\n",
    "    # prefer exact-ish matches first\n",
    "    candidates = []\n",
    "    for p in folder.glob('*'):\n",
    "        if p.is_file() and key_l in p.name.lower():\n",
    "            candidates.append(p)\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"Could not find a file containing '{key}' in {folder.resolve()}\")\n",
    "    # Prefer files with .csv/.xlsx/.xls extensions\n",
    "    preferred_ext = {'.csv': 0, '.xlsx': 1, '.xls': 2, '': 3}\n",
    "    candidates.sort(key=lambda p: preferred_ext.get(p.suffix.lower(), 9))\n",
    "    return candidates[0]\n",
    "\n",
    "def smart_read_table(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read a 'CSV' that might actually be an Excel file. Works for .csv/.xlsx/.xls/no-extension.\"\"\"\n",
    "    suffix = path.suffix.lower()\n",
    "    # If it's clearly excel, read excel\n",
    "    if suffix in {'.xlsx', '.xls'}:\n",
    "        return pd.read_excel(path)\n",
    "    # If it's clearly csv, try csv with encoding fallbacks; if that fails, try excel\n",
    "    if suffix == '.csv':\n",
    "        for enc in ['utf-8', 'utf-8-sig', 'cp1252', 'latin1']:\n",
    "            try:\n",
    "                return pd.read_csv(path, encoding=enc)\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "            except Exception:\n",
    "                break\n",
    "        # last resort: try excel (some 'csv' downloads are actually excel)\n",
    "        return pd.read_excel(path)\n",
    "\n",
    "    # No extension or unknown: try excel first (Windows often saves as Excel)\n",
    "    try:\n",
    "        return pd.read_excel(path)\n",
    "    except Exception:\n",
    "        # fallback to csv attempts\n",
    "        for enc in ['utf-8', 'utf-8-sig', 'cp1252', 'latin1']:\n",
    "            try:\n",
    "                return pd.read_csv(path, encoding=enc)\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        # final fallback\n",
    "        return pd.read_csv(path, encoding_errors='ignore')\n",
    "\n",
    "def parse_date_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Parse dates with preferred formats to avoid pandas 'could not infer format' warnings.\"\"\"\n",
    "    d = pd.to_datetime(s, format='%Y-%m-%d', errors='coerce')\n",
    "    if d.isna().any():\n",
    "        # fallback for occasional formats like 28-Dec-18\n",
    "        d2 = pd.to_datetime(s, format='%d-%b-%y', errors='coerce')\n",
    "        d = d.fillna(d2)\n",
    "    # final fallback\n",
    "    if d.isna().any():\n",
    "        d3 = pd.to_datetime(s, errors='coerce')\n",
    "        d = d.fillna(d3)\n",
    "    return d\n",
    "\n",
    "def load_stock(ticker: str) -> pd.DataFrame:\n",
    "    path = find_file_for_key(ticker, data_dir)\n",
    "    df = smart_read_table(path)\n",
    "    if 'Date' not in df.columns:\n",
    "        raise ValueError(f\"{ticker}: 'Date' column not found in {path.name}\")\n",
    "    df['Date'] = parse_date_series(df['Date'])\n",
    "    # Coerce numeric columns if present\n",
    "    for c in ['Open','High','Low','Close','Adj Close','Volume']:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    df = df.dropna(subset=['Date','Close','Adj Close']).sort_values('Date').set_index('Date')\n",
    "    df = df.loc['2018-01-01':'2018-12-31', ['Close','Adj Close']]\n",
    "    return df\n",
    "\n",
    "def load_usdjpy() -> pd.Series:\n",
    "    path = find_file_for_key('usdjpy', data_dir)\n",
    "    fx = smart_read_table(path)\n",
    "    if 'Date' not in fx.columns or 'Close' not in fx.columns:\n",
    "        raise ValueError(f\"USDJPY: need 'Date' and 'Close' columns in {path.name}\")\n",
    "    fx['Date'] = parse_date_series(fx['Date'])\n",
    "    fx['Close'] = pd.to_numeric(fx['Close'], errors='coerce')\n",
    "    fx = fx.dropna(subset=['Date','Close']).sort_values('Date').set_index('Date')\n",
    "    fx = fx.loc['2018-01-01':'2018-12-31', ['Close']].dropna()\n",
    "    return fx['Close'].rename('USDJPY')\n",
    "\n",
    "# Quick check: show what files were found\n",
    "print('Data directory:', data_dir.resolve())\n",
    "for key in universe + ['usdjpy']:\n",
    "    p = find_file_for_key(key, data_dir)\n",
    "    print(f\"{key:6s} -> {p.name} (suffix='{p.suffix}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a33d79f",
   "metadata": {},
   "source": [
    "## 1) Load 10 stocks and build Close/Adj Close matrices (aligned dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb9fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = {tk: load_stock(tk) for tk in universe}\n",
    "\n",
    "# Align all dates (intersection) so the matrices are consistent\n",
    "date_index = None\n",
    "for tk, df in stocks.items():\n",
    "    date_index = df.index if date_index is None else date_index.intersection(df.index)\n",
    "date_index = date_index.sort_values()\n",
    "\n",
    "close_df = pd.DataFrame({tk: stocks[tk].loc[date_index, 'Close'] for tk in universe}, index=date_index)\n",
    "adj_df   = pd.DataFrame({tk: stocks[tk].loc[date_index, 'Adj Close'] for tk in universe}, index=date_index)\n",
    "\n",
    "close_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c5620c",
   "metadata": {},
   "source": [
    "## 2) Compute dividend per share (from Close vs Adj Close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d2ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dividend_per_share(close_s: pd.Series, adj_s: pd.Series, diff_thresh: float = 0.001) -> pd.Series:\n",
    "    r_close = close_s.shift(1) / close_s\n",
    "    r_adj = adj_s.shift(1) / adj_s\n",
    "    diff = r_close - r_adj\n",
    "    div = diff * close_s\n",
    "    div = div.where(diff.abs() > diff_thresh, 0.0)\n",
    "    div = div.where(div > 0, 0.0)\n",
    "    return div.fillna(0.0)\n",
    "\n",
    "div_per_share = pd.DataFrame({tk: compute_dividend_per_share(close_df[tk], adj_df[tk]) for tk in universe}, index=date_index)\n",
    "\n",
    "# Show some detected dividends\n",
    "for tk in ['IBM','MSFT','AAPL','ORCL','SAP']:\n",
    "    nz = div_per_share.loc[div_per_share[tk] > 0, [tk]]\n",
    "    print(tk, 'dividend days:', len(nz))\n",
    "    display(nz.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaaa896",
   "metadata": {},
   "source": [
    "## 3) Portfolio engine + 5-day strategies (buy_low / buy_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c866fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_CASH = 5_000_000\n",
    "INITIAL_PICK = ['IBM','MSFT','GOOG','AAPL','AMZN']  # per project example\n",
    "\n",
    "def buy_equal_allocation(cash: float, date: pd.Timestamp, tickers: list[str]) -> tuple[dict, float]:\n",
    "    allocation = cash / len(tickers)\n",
    "    holdings = {}\n",
    "    leftover = 0.0\n",
    "    for tk in tickers:\n",
    "        price = float(close_df.loc[date, tk])\n",
    "        shares = int(allocation // price)\n",
    "        cost = shares * price\n",
    "        holdings[tk] = shares\n",
    "        leftover += allocation - cost\n",
    "    return holdings, leftover\n",
    "\n",
    "def holdings_value(holdings: dict, date: pd.Timestamp) -> float:\n",
    "    return sum(shares * float(close_df.loc[date, tk]) for tk, shares in holdings.items())\n",
    "\n",
    "def run_strategy(rebalance_days: int = 5, mode: str = 'buy_low'):\n",
    "    assert mode in {'buy_low','buy_high'}\n",
    "    start_date = pd.Timestamp('2018-01-02')\n",
    "    if start_date not in date_index:\n",
    "        start_date = date_index[0]\n",
    "\n",
    "    cash = float(INITIAL_CASH)\n",
    "    holdings, cash = buy_equal_allocation(cash, start_date, INITIAL_PICK)\n",
    "\n",
    "    mtm = []\n",
    "    last_reb_date = start_date\n",
    "\n",
    "    # rebalance dates based on business-day index\n",
    "    rebalance_dates = set()\n",
    "    start_pos = date_index.get_loc(start_date)\n",
    "    for pos in range(start_pos + rebalance_days, len(date_index), rebalance_days):\n",
    "        rebalance_dates.add(date_index[pos])\n",
    "\n",
    "    for d in date_index:\n",
    "        # dividends credited BEFORE trading on day d\n",
    "        cash += sum(shares * float(div_per_share.loc[d, tk]) for tk, shares in holdings.items())\n",
    "\n",
    "        # rebalance if needed\n",
    "        if d in rebalance_dates:\n",
    "            pct = (adj_df.loc[d] / adj_df.loc[last_reb_date]) - 1.0\n",
    "            picks = (pct.sort_values().head(5) if mode == 'buy_low' else pct.sort_values(ascending=False).head(5)).index.tolist()\n",
    "\n",
    "            # sell all, then buy picks\n",
    "            cash += holdings_value(holdings, d)\n",
    "            holdings = {}\n",
    "            holdings, cash = buy_equal_allocation(cash, d, picks)\n",
    "            last_reb_date = d\n",
    "\n",
    "        mtm.append(cash + holdings_value(holdings, d))\n",
    "\n",
    "    return pd.Series(mtm, index=date_index, name=f\"MTM_{mode}_{rebalance_days}d\")\n",
    "\n",
    "\n",
    "mtm_low_5  = run_strategy(5, 'buy_low')\n",
    "mtm_high_5 = run_strategy(5, 'buy_high')\n",
    "\n",
    "mtm_low_5.tail(1), mtm_high_5.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5112963f",
   "metadata": {},
   "source": [
    "## 4) High Tech Index + plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6576998",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_tech_index = close_df.mean(axis=1).rename('HighTechIndex')\n",
    "\n",
    "def norm(s: pd.Series) -> pd.Series:\n",
    "    return s / s.iloc[0]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(norm(mtm_low_5).index, norm(mtm_low_5).values, label='MTM buy_low (5d)')\n",
    "plt.plot(norm(mtm_high_5).index, norm(mtm_high_5).values, label='MTM buy_high (5d)')\n",
    "plt.plot(norm(high_tech_index).index, norm(high_tech_index).values, label='High Tech Index')\n",
    "plt.title('Normalized Performance (Start = 1.0)')\n",
    "plt.xlabel('Date'); plt.ylabel('Normalized Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b053989",
   "metadata": {},
   "source": [
    "## 5) USDJPY conversion (robust loader — no UnicodeDecodeError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8547db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "usd_jpy = load_usdjpy()\n",
    "usd_jpy_aligned = usd_jpy.reindex(date_index).ffill()\n",
    "\n",
    "mtm_low_5_jpy = (mtm_low_5 * usd_jpy_aligned).rename('MTM_buy_low_5d_JPY')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(norm(mtm_low_5).index, norm(mtm_low_5).values, label='MTM USD (buy_low 5d)')\n",
    "plt.plot(norm(mtm_low_5_jpy).index, norm(mtm_low_5_jpy).values, label='MTM JPY (buy_low 5d)')\n",
    "plt.title('MTM in USD vs JPY (Normalized)')\n",
    "plt.xlabel('Date'); plt.ylabel('Normalized Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mtm_low_5.tail(1), mtm_low_5_jpy.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78d6524",
   "metadata": {},
   "source": [
    "## 6) Optimize rebalance interval (1–20 business days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e9c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_interval(mode: str, min_days: int = 1, max_days: int = 20):\n",
    "    rows = []\n",
    "    for d in range(min_days, max_days + 1):\n",
    "        mtm = run_strategy(d, mode)\n",
    "        rows.append((d, float(mtm.iloc[-1])))\n",
    "    return pd.DataFrame(rows, columns=['rebalance_days','final_MTM_USD']).sort_values('final_MTM_USD', ascending=False)\n",
    "\n",
    "opt_low = optimize_interval('buy_low', 1, 20)\n",
    "opt_high = optimize_interval('buy_high', 1, 20)\n",
    "\n",
    "display(opt_low.head(10))\n",
    "display(opt_high.head(10))\n",
    "\n",
    "best_low = opt_low.iloc[0]\n",
    "best_high = opt_high.iloc[0]\n",
    "best_low, best_high"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
